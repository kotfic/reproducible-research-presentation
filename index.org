#+Title: Reproducible research: open access, open data, open tools and metrics
#+Author: Christopher Kotfila
#+Email: ckotfila@albany.edu
#+OPTIONS: toc:nil num:nil

#+REVEAL_ROOT: revealjs/
#+REVEAL_HLEVEL: 1

* Introduction
Christopher Kotfila\\
PhD student - Information Science Department\\
College of Computing and Information\\
ckotfila@albany.edu \\
@kotfic
** Who Am I?
#+BEGIN_NOTES
Graduated last spring with an MSIS from here at UAlbany

Focused primarily on scholarly communication and open access

Last 10 years -amature/professional software developer

both in private sector doing web application development

and in academia as a research assistant.
#+END_NOTES
** That's a long title
Are you really going to talk about all that in 30 minutes?

#+BEGIN_NOTES
So the answer of course is no.

This talk is going to focus on reproducible research

cast it in the light of some problems that software engineers have been facing for a number of years

and then really what I'm really hoping is that I'll get a chance to show you some really cool tools that have emerged in the last 5 to 10 years

and then talk about how these tools related to open access and open data. 
#+END_NOTES

** Caveat 
#+BEGIN_QUOTE
"Release early release often"[1]
#+END_QUOTE
 
#+BEGIN_HTML
<sup> [1] Eric S. Raymond, The Cathedral and the Bazaar (1997) </sup>
#+END_HTML

#+BEGIN_NOTES
So there is a relatively well established philosophy in software development "Release early release often"

In software this means,  get your program out there as soon as possible and let people start giving you feedback

This presentation emerges from a number of threads that have been stewing for the last couple of years

The ideas here are still a little rough,  in true Open Source fashion,  I hope you'll help me refine them. 

#+END_NOTES
* An Example from personal experience
#+BEGIN_NOTES
So - I began working as a research assistant here at Albany at the beginning of this summer

The work I'm learning to do involves writing computer programs to to pull structured information out of unstructured medical discharge summaries.  

There are a lot of interesting and practical applications for work like this

tracking patient outcomes for instance

identifying systematic issues at hospitals or with health care processes - lots of stuff.

This is done primarily with natural language process tools (ie.  computer programs and processes) 

As an initial exercize I set out to reproduce a well performing system in a particular challenge that tried to identify whether patients had obesity based on the free-form text in their medical record.

#+END_NOTES
* The idea was...
** reproduce the system based on the description in the paper,

** try to identify systematic errors
** correct those errors
** publish in a prestigious journal
** get a tenure-track position at a respected university
** live happily ever afer

* So It turns out...
#+BEGIN_NOTES
You can probably guess from the title of the talk,  this did not go as planned.

The system itself is gone - emailed the authors,  they were extreamly helpful

but it was stored on a server they no longer have access too.

the system, the solution in its most tangible form, is gone.

This brings us to

#+END_NOTES



* Reproducibility
#+BEGIN_NOTES
Fundimentally rooted in scientific skepticism. 

Findings may be interesting - but if they can't be reproduced by third parties in normalalized settings 

Then we're obligated to "take the word of" researchers 

rather than building on a firm foundation of findings that meet the bar of reproducibility
#+END_NOTES

* Reproducibility creates Credibility 

* Technology enables reproducibility 
#+BEGIN_NOTES
Technology has traditionally enabled reproducibility
#+END_NOTES
* 
[[file:img/402px-1665_phil_trans_vol_i_title.png]]

#+BEGIN_NOTES
our modern day style of scholarly communication really begins with the introduction of the printing press into the scholarly process

The wide distribution of ideas allowed for scholars across Europe to test and refine the ideas of their peers. 

Today we face a new distribution mechanism: 

#+END_NOTES


* The Internet
** TODO finish notes and trasition for this slide
#+BEGIN_NOTES

Printing press is the technology that enables the goals of reproducability (as Shirky poitns out - some 150 years later)
Internet and modern computing is the technology that creates the current threat to the =Credability= of the research process
But Technology can also help to solve the problem it is creating.
#+END_NOTES

* Computational Science
#+BEGIN_NOTES
While publishers and librarians come to grips with how to best utilize this new distribution mechanism (ie. Open Access)

Researchers have been struggling with how to ensure reproducibility in an environment where computation is becoming the lingua franca. 

#+END_NOTES


* The Open Science Pipeline 
#+BEGIN_NOTES
Really reproducible research spans all the steps between a data source and a publiction. 

All the steps involved in obtaining,  scrubing, modeling, interpreting and presenting data. 

This doesn't HAVE to be in the context of an open data and open access 

BUT - in this context open reproducible research brings end-to-end transparency to the entire process.

#+END_NOTES
+ Open Data
+ Reproducible Research
+ Open Access


* 
#+BEGIN_QUOTE
“The idea is: An article about computational science in a scientific
publication is not the scholarship itself, it is merely advertising of the
scholarship. The actual scholarship is the complete ... set of
instructions [and data] which generated the figures.”
David Donoho, 1998.
#+END_QUOTE

#+BEGIN_NOTES
David Donoho is one of the reseachers over at Stanford who has been actively involved with the reproducibility movement for decades

As Donoho puts it:

His quote here really captures the kind of reproducibility that is creating what some are describing as a credibility crisis
#+END_NOTES

* A state of "Crisis"
+ Begley, C. Glenn and Ellis, Lee M. Drug development: Raise standards for preclinical cancer research, Nature (2012)
+ AAAS (2011). AAAS annual meeting: Workshop on the digitization of science: Repro-ducibility and interdisciplinary knowledge transfer.
+ AMP (2011). Applied mathematics perspectives workshop on reproducible research: Toolsand strategies for scientific computing applied mathematics perspectives.
+ SIAM-CSE (2011). SIAM conference on computational science & engineering workshop onverifiable, reproducible computational science.
+ SIAM-Geo (2011). SIAM geosciences workshop on reproducible science and open-sourcesoftware in the geosciences
+ NSF (2010). National science foundation workshop on changing the conduct of science inthe information age summary.
+ ENAR (2011). Research ethics in biostatistics: Invited panel discussion at ENAR 2011 onthe biostatistician’s role in reproducible research.
+ Yale Law School Round Table on Data and Code Sharing (2009)

#+BEGIN_NOTES
Many of these conferences, papers, and authors have proposed tentative solutions that span a spectrum 

from technical implementations of code repositories 

to policy recommendations that seek to support socio-cultural change in the academic community.
#+END_NOTES


* Some Examples
+ [[http://runmycode.org/][RunMyCode.org]]
+ [[http://www.stat.uni-muenchen.de/~leisch/Sweave/][Sweave]]
+ [[https://openscienceframework.org/][Open Science Framework]]
+ [[http://www.stanford.edu/~vcs/Papers.html][Victoria Stodden's work]]

#+BEGIN_NOTES
Of course there are many many more examples 

but what I'd like to humbly suggest,  is that at least from a technical perspective,  this is already a solved problem.

And - that there are STRONG examples in the open source software development community

for how resolve the socio-cultural aspects of the computational reproducibility problem.

#+END_NOTES


* Open Source Software Developers
#+BEGIN_NOTES
Reproducibility is (arguably) the number one concern of open source software developers.

As an software developer,  if the code i write and run on my system 

doesn't run on your system

my code is broken.

And they've been building tools to support reproducibility for the last 30 or 40 years. 
#+END_NOTES


* Open Source Software Developers and Researchers 
(have a lot in common)
** Highly specialized
** Experts in their area
** Collaborate frequently
Usually working on teams that are \\
geographically disparate  \\ 
culturally diverse
** Produce complex processes that 
+ obtain 
+ scrub
+ explore
+ model 
+ interpret
+ and display data
** Code and Documentation
#+BEGIN_NOTES
For software developers documentation tends to take a back seat to code 

For researchers code (and data processesing) tends to take a back seat to documentation

and by documentation i mean a publishible paper. 
#+END_NOTES

* Tools

* Source Control Management System
** Maybe you've seen this problem before

** =Who= did =what=, =when= and =why=?
* Git and Github
** The details of git and github are not vital to understand importaint right now
** Forking for Fun and Profit
** SCMS are not just for Source Code
https://github.com/blog/1657-introducing-government-github-com

* Executable Papers and Literate Programing
** Examples

#+BEGIN_SRC R :results graphics :file img/graph.png :exports results
# Define 2 vectors
cars <- c(1, 3, 6, 4, 9)
trucks <- c(2, 5, 4, 5, 12)

# Graph cars using a y axis that ranges from 0 to 12
plot(cars, type="o", col="blue", ylim=c(0,12))

# Graph trucks with red dashed line and square points
lines(trucks, type="o", pch=22, lty=2, col="red")

# Create a title with a red, bold/italic font
title(main="Autos", col.main="red", font.main=4)

#+END_SRC


* Oh yeah... The Metrics


* Final Thought


* Epilogue: Engaging with the presentation
"A mini-tutorail" 

* Parking lot                                                      :noexport:
** Open Access
**** Copyright exists to incentiveze creative works of non-trival effort
**** For scholars, incentive structure for publication is different
**** Attribution still a key factor
**** Prestige infrastructure 
** Software-carpentry
http://software-carpentry.org/
** Science Code Manifesto
http://sciencecodemanifesto.org/
**** Code
All source code written specifically to process data for a published paper must be available to the reviewers and readers of the paper.
**** Copyright
The copyright ownership and license of any released source code must be clearly stated.
**** Citation
Researchers who use or adapt science source code in their research must credit the code’s creators in resulting publications.
**** Credit
Software contributions must be included in systems of scientific assessment, credit, and recognition.
**** Curation
Source code must remain available, linked to related materials, for the useful lifetime of the publication.

** Modern Reproducible research
** Who is doing this?
**** Stanford Group
Jon Claerbout \\
David Donoho
**** Literate Programing
Donald Knuth
**** Bioinformatics and statistically intensive biology
**** Computational Statisticians and the R Community
Friedrich Leisch
**** Emacs and Org-Babel Community
**** Climate Code Foundation
Nick Barnes

#+BEING_NOTES
Relatively new group but with several high profile articles

Guy behind Science Code Manifesto
http://www.nature.com/news/2010/101013/full/467753a.html
#+END_NOTES
**** Elsiver?
http://www.executablepapers.com/




** Linus's Law
"given enough eyeballs, all bugs are shallow"; \\ 
or more formally: \\
"Given a large enough beta-tester and co-developer base, almost every problem will be characterized quickly and the fix will be obvious to someone." 
[citeRaymond]

** Government Github
** Provisions, Quid pro quos
+ Qualitative research,  research that lends itself to statistical analysis,  or requires any kind of data transformation before being analyzed.
** Reproducible research as bridge
+ Open Data provides direct access to data sources that have been created by all types of institutions (Gov, academic, private sector)
  - Published in many different ways, Raw data files,  API, SPARQL, sometimes this data is useful,  sometimes it is garbage
+ Open Access "advertises" the research,  but isn't the research itself.
  - Will need to do obligatory introduction to reproducible as foundation of modern science
  - Open Access is absolutely vital - it raises the visibility of a paper, research, and improves over all impact [citation?] [fn:1] 
+ The ACTUAL research is the transformed data, and the analysis - sometimes this means data cleanup,  sometimes this means large scale data transformation pipelines like in NLP and machine learning
  - reproducible research is the bridge between open data and open access it takes open data as an input, and produces papers as an output

  - shifting from a "publication as research" model the [data + transformation code + analysis + interpretation] as the research changes the meaning of open access to research

  - This begs an important question - how do we publish this kind of "document"

  - Not restrained by old journal model - can skip growing pains of online journal model (analogy of 1960's movies that were just filmed theater shows?)

  - Some have already discussed blogs, social networking etc as possible outcomes,  but metrics in this environment are still emerging

  - Metrics in the field of software engineering are pretty good though,

  - In broad strokes,  software engineers produce code and documentation,   researchers produce documentation (in the form of publications)  and code. 
    - software engineers even have a counterpart to Reproducible research - literate programming.
  - Key component of reproducible is access to the tools that produced to code and documentation - tie in open source

** Tools and Metrics
+ Github and github style metrics
+ Sweave
+ Emacs w/org-mode and org-babel
+ Things out there that are interesting,  but don't quite get the job done
  - Google Docs (highly collaborate, no version control,  no code integration) 
  - Authorea - (highly collaborative,  focus on academic's needs) 
  - runmycode.org ( code but no direct integration with "documentation", collaborative but not quite like github)
+ requirements for the ideal process
  - Open source tools (so barrier to access to those tools is only technical)
  - allows collaboration across researchers
  - embeds research (ie. code) directly into the documentation. 
  - version control (for archiving!) 


[fn:1] This could provide an interesting example for github style issue tracking, someone notes that this claim needs a citation,  author finds citation and adds it into the documents 


* Tasks                                                            :noexport:
** Archive                                                         :ARCHIVE:
*** DONE find quote about the paper being an advertisement for the reserch
CLOSED: [2013-10-07 Mon 20:46]
:PROPERTIES:
:ARCHIVE_TIME: 2013-10-07 Mon 20:46
:END:
** DONE Read Claerbout's history of reproducible research
CLOSED: [2013-10-20 Sun 10:23]
[[http://sepwww.stanford.edu/data/media/public/sep//jon/reproducible.html][History of Reproducible Research]]

** DONE Scopus Claerbout's stuff on RR
CLOSED: [2013-10-20 Sun 10:23]
** TODO Read more about this Elsiver executible paper competition
http://www.executablepapers.com/
** TODO more information about Sweave
** TODO get more info about SPARQL
** TODO Does Authorea fit in here?
https://www.authorea.com/


* Papers to Read                                                   :noexport:
:PROPERTIES:
:ID:       487c95e9-eafe-46bd-882f-65cfc8aff174
:END:
+ [[id:cae658a7-daf9-44aa-b4d7-9fe44eaf907b][Stodden, V. :: Reproducible research: Tools and strategies for scientific computing (2012)]]
+ [[id:67c28701-807e-4fac-9f1b-cc5562ed0207][Stodden, Victoria :: Enabling reproducible research: licensing for scientific innovation (2009)]]
+ [[id:a54a04a8-aa72-45b5-bd93-6835e948357a][Knuth, Donald Ervin :: Literate programming (1984)]]

+ [[id:39da92be-f1e4-48e4-8efd-7711e53a958d][Hothorn, Torsten and Leisch, Friedrich :: Case studies in reproducibility (2011)]]
+ [[id:f4bdd44c-833e-4f7f-b752-3ee8bc92df9d][Peng, Roger D. :: Reproducible Research in Computational Science (2011)]]
+ [[id:358b6e1e-0898-4ef9-8074-4e869fa5774b][David Donoho and Arian Maleki and Inam Rahman and Morteza Shahram and Victoria Stodden :: 15 Years of Reproducible Research in Computational Harmonic Analysis (2008)]]
+ [[id:d7300347-3a70-4d70-aea6-e7781136c6b0][Schulte, E. and Davison, D. and Dye, T. and Dominik, C. :: A multi-language computing environment for literate programming and reproducible research (2012)]]


+ [[id:84ecf889-4619-4efc-bd45-fc48d026619b][Baiocchi, G. :: Reproducible research in computational economics: Guidelines, integrated approaches, and open source software (2007)]]
+ [[id:c7581914-7560-40a2-856e-15a987daa778][Van Gorp, P. and Mazanek, S. :: SHARE: A web portal for creating and sharing executable research papers (2011)]]
+ [[id:9c4dbbea-0442-4bf7-a52e-af8298698677][Mesirov, J.P. :: Accessible reproducible research (2010)]]     

+ [[id:53e934d9-9567-4cc6-aa83-4ebb7102763f][Vandewalle, P. and Kovacević, J. and Vetterli, M. :: Reproducible research in signal processing: What, why, and how (2009)]]

+ [[id:38a3e14d-d9ec-4b01-b315-a778caa59573][Fomel, S. and Claerbout, J.F. :: Guest editors' introduction: Reproducible research (2009)]]

M. Schwab, N. Karrenbach, J. Claerbout, Making scientific computations reproducible. Comput. Sci. Eng. 2, 61 (2000). Search Google Scholar
C. Laine, S. N. Goodman, M. E. Griswold, H. C. Sox, Reproducible research: Moving toward research the public can really trust. Ann. Intern. Med. 146, 450 (2007). Medline
G. King, Replication, Replication. PS: Polit. Sci. Polit. 28, 444 (1995). CrossRef

+ [[id:a9ab7962-3615-4b49-b460-45bd2c876c4c][Tomi Kauppinen and Giovana Mira de Espindola :: Linked Open Science-Communicating, Sharing and Evaluating Data, Methods and Results for Executable Papers  (2011)]]

** Nature articles
:PROPERTIES:
:ID:       642c4d43-1300-42ba-acc7-35c1d3e5901f
:END:
+ [[id:626ecf83-ad7f-429e-b036-84b10c1c4fe1][Begley, C. Glenn and Ellis, Lee M. :: {Drug development: Raise standards for preclinical cancer research} (2012)]]
+ [[id:066d8daf-ef7e-4efe-82c6-bf044e12e316][Mobley, , Aaron AND Linder, , Suzanne K. AND Braeuer, , Russell AND Ellis, , Lee M. AND Zwelling, , Leonard :: A Survey on Data Reproducibility in Cancer Research Provides Insights into Our Limited Ability to Translate Findings from the Laboratory to the Clinic (2013)]]






